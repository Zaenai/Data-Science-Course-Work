{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70a67b48",
   "metadata": {},
   "source": [
    "# Verbatim opgavebeskrivelse\n",
    "# Milestone 2\n",
    "DESCRIPTION\n",
    "\n",
    "After cleaning and processing our data in the first milestone, Milestone 2 will focus on how to efficiently represent the data in a database. Like last time, the milestone takes the form of a short jupyter notebook. It must be handed in on Friday, April 29, at 16:00 (in groups), and it is a requirement for attending the exam (it will be evaluated as passed/fail).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3166a8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af9342a7",
   "metadata": {},
   "source": [
    "# Task 1.\n",
    "\n",
    "The first task is to demonstrate that you have a working database containing the FakeNewsCorpus dataset. Explain your choice of schema design. You have been working on this task on a small subset of the data during the TA-sessions. For this milestone, demonstrate that your database contains a larger number of rows (e.g. one million - or however many you can reasonably work with on your available hardware), and that it supports simple queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0323a9fa",
   "metadata": {},
   "source": [
    "We chose to go with a fairly simple schema design where we primarely split out the one to many relations tags and keywords. We chose to do the many to many relation between authors and articles, but this could have been done with urls/domains as well. \n",
    "\n",
    "<img src=\"img/Screenshot_3.png\" width=\"800\" height=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2e49a6",
   "metadata": {},
   "source": [
    "Following code is what we used to create the csv files for the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd07546f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# psh270, jxs535, fgp424, hkp680\n",
    "import nltk as nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('corpus')\n",
    "nltk.download('all')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from cleantext.sklearn import CleanTransformer # likely required to ´pip install clean-text´\n",
    "from cleantext import clean\n",
    "from pathlib import Path\n",
    "data = pd.read_csv(\"https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv\")\n",
    "#data = pd.read_csv(r\"C:\\Users\\Computer\\Documents\\GitHub\\Data-Science-Course-Work\\CSVs\\1mio-raw.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9434dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_useless_data(data):\n",
    "    #Dropping unneeded columns\n",
    "    cols_to_delete = [\"Unnamed: 0\"]\n",
    "    for column in data.columns:\n",
    "        if data[column].isnull().values.all():\n",
    "            cols_to_delete.append(column)\n",
    "    data.drop(cols_to_delete, 1, inplace=True)\n",
    "    \n",
    "    #Dropping entries with nan type\n",
    "    data.dropna(subset = [\"type\"], inplace = True)\n",
    "    #Dropping entries with unknown type\n",
    "    data.drop(data.loc[data[\"type\"] == \"unknown\"].index, inplace=True)\n",
    "\n",
    "drop_useless_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55eea0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_keyword_list = []\n",
    "keyword_id = 0\n",
    "article_author_list = [] #Ends up containing lists [article id, author name]\n",
    "author_id = 0\n",
    "tag_list = []\n",
    "tags_id = 0\n",
    "articles = []\n",
    "has_written_list = []\n",
    "\n",
    "\n",
    "def string_to_list(s):\n",
    "    l = s.strip('][').split(', ')\n",
    "    return [w.strip(\"'\").lower() for w in l]\n",
    "\n",
    "for i, article in data.iterrows():\n",
    "    article_id = int(article[\"id\"])\n",
    "\n",
    "    if article[\"meta_keywords\"] == \"['']\":\n",
    "        article_keyword_list.append([keyword_id, article_id, np.nan])\n",
    "        keyword_id += 1\n",
    "    else:\n",
    "        for word in string_to_list(article[\"meta_keywords\"]):\n",
    "            article_keyword_list.append([keyword_id, article_id, word])\n",
    "            keyword_id += 1\n",
    "    \n",
    "    if str(article[\"authors\"]) == \"nan\":\n",
    "        article_author_list.append([author_id, article_id, np.nan])\n",
    "        author_id += 1\n",
    "    else:\n",
    "        for author in string_to_list(article[\"authors\"]):\n",
    "            article_author_list.append([author_id, article_id, author])\n",
    "            has_written_list.append([author_id, article_id])\n",
    "            author_id += 1\n",
    "\n",
    "    if str(article[\"tags\"]) == \"nan\":\n",
    "        tag_list.append([tags_id, article_id, np.nan])\n",
    "        tags_id += 1\n",
    "    else:\n",
    "        for tag in string_to_list(article[\"tags\"]):\n",
    "            tag_list.append([tags_id, article_id, tag])\n",
    "            tags_id += 1\n",
    "\n",
    "    clean = article[\"content\"].replace(\"\\n\", \"\")\n",
    "    cleaned = clean.replace('\"', \"\")\n",
    "    articles.append([article[\"id\"],\n",
    "                    article[\"domain\"], \n",
    "                    article[\"type\"],\n",
    "                    article[\"url\"],  \n",
    "                    cleaned,\n",
    "                    article[\"scraped_at\"],\n",
    "                    article[\"inserted_at\"],\n",
    "                    article[\"updated_at\"], \n",
    "                    article[\"title\"],  \n",
    "                    article[\"meta_description\"]])\n",
    "\n",
    "keywords_df = pd.DataFrame(article_keyword_list, columns=[\"id\", \"article_id\", \"keyword\"])\n",
    "authors_df = pd.DataFrame(article_author_list, columns=[\"id\", \"article_id\", \"author_name\"])\n",
    "tags_df = pd.DataFrame(tag_list, columns=[\"id\", \"article_id\", \"tag\"])\n",
    "article_df = pd.DataFrame(articles, columns=[\"article_id\", \"domain\", \"type\", \"url\", \"content\", \"scraped_at\", \"inserted_at\", \"updated_at\", \"title\", \"meta_description\"])\n",
    "has_written_df = pd.DataFrame(has_written_list, columns=[\"author_id\", \"article_id\"])\n",
    "\n",
    "keyword_combinations_path = Path(r\"C:\\Users\\Peter\\Ny mappe\\keywords.csv\")\n",
    "authors_path = Path(r\"C:\\Users\\Peter\\Ny mappe\\authors.csv\")\n",
    "tags_path = Path(r\"C:\\Users\\Peter\\Ny mappe\\tags.csv\")\n",
    "articles_path = Path(r\"C:\\Users\\Peter\\Ny mappe\\articles.csv\")\n",
    "has_written_path = Path(r\"C:\\Users\\Peter\\Ny mappe\\has_written.csv\")\n",
    "\n",
    "keywords_df.to_csv(keyword_combinations_path, index=False)\n",
    "authors_df.to_csv(authors_path, index=False)\n",
    "tags_df.to_csv(tags_path, index=False)\n",
    "article_df.to_csv(articles_path, index=False)\n",
    "has_written_df.to_csv(has_written_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8014ce58",
   "metadata": {},
   "source": [
    "Following screenshots are simple sql queries made to the database.\n",
    "\n",
    "<img src=\"img/unknown.png\" width=\"400\" height=\"400\"> <img src=\"img/Screenshot_2.png\" width=\"500\" height=\"400\"> <img src=\"img/Screenshot_1.png\" width=\"800\" height=\"200\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec60f890",
   "metadata": {},
   "source": [
    "# Task 2.\n",
    "\n",
    "List the relations you have created in your database. For each relation:\n",
    "\n",
    "list its attributes\n",
    "list its functional dependencies.\n",
    "list all the primary keys.\n",
    "Is each relation in BCNF form? If not, show how to transform the tables in BCNF and explain why it might be better (or not) to use the BCNF relations in your database.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fae360",
   "metadata": {},
   "source": [
    "We have the following relations.\n",
    "1. Authors to article which is a many to many relation.\n",
    "        A. This relation covers does not have attributes in the join table, but does match the primary key of articles and authors.\n",
    "        B. We have a non trivial dependency as if we know the author we know what article id's they have written and the other way around if we know the article id we know the authors while them not being a subset of eachother.\n",
    "        C. Primary keys in this relation is author_id and article_id\n",
    "\n",
    "2. Keywords to article which is a many to one relation.\n",
    "        A. This relation only talks to one atribute being a one to many relation. That being article_id\n",
    "        B. This is again a non trivial dependency as if we know article id we know the corrosponding keywords. Again that keywords is not a subset of article id. \n",
    "        C. Only one primary key is used in a one to many relation that being the article_id\n",
    "\n",
    "3. Tags to article which is a many to one relation.\n",
    "        A. This relation only talks to one atribute being a one to many relation. That being article_id\n",
    "        B. This is again a non trivial dependency as if we know article id we know the corrosponding tags. Again that tags is not a subset of article id. \n",
    "        C. Only one primary key is used in a one to many relation that being the article_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24398f83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a3b20a0",
   "metadata": {},
   "source": [
    "# Task 3.\n",
    "\n",
    "Once your database is loaded, you can start issuing queries to better understand the characteristics of the data. Formulate the following queries in the database languages requested (in the square brackets following each item) and briefly discuss what you observe when you execute them over your database:   \n",
    "\n",
    "List the domains of news articles of reliable type and scraped at or after January 15, 2018. NOTE: Do not include duplicate domains in your answer. [Languages: relational algebra and SQL]\n",
    "List the name(s) of the most prolific author(s) of news articles of fake type. An author is among the most prolific if it has authored as many or more fake news articles as any other author in the dataset. [Languages: extended relational algebra and SQL]\n",
    "Count the pairs of article IDs that exhibit the exact same set of meta-keywords, but only return the pairs where the set of meta-keywords is not empty. [Language: SQL]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8c1689",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65270c5c",
   "metadata": {},
   "source": [
    "# Task 4.\n",
    "\n",
    "Now that we have our data in a database, let’s revisit the “interesting observations” task from Milestone 1 - but now using queries to the database. The idea is to write database queries (e.g. using GROUP BY and COUNT) that explore features of the data set that are relevant to the fake news prediction task: outliers, artefacts. It’s OK to investigate the same issues as in Milestone 1 (now using database queries) - but you are also very welcome to come up with completely new queries. You should write at least 3 such queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4232286f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03de5ca5",
   "metadata": {},
   "source": [
    "# Task 5.\n",
    "\n",
    "Just like last time, after the hand in deadline, each group will be asked to evaluate the work of three other groups, based on a short list of criteria that you can find within the peergrade system. Again, this will only work well if everyone puts some effort into providing constructive comments, so please allocate some time to do this properly. It is an opportunity to get some feedback that can help you improve your final project. The deadline for giving feedback is a week after the hand-in deadline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2700606",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35298891",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dddd9ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
