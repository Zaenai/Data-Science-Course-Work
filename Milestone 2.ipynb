{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70a67b48",
   "metadata": {},
   "source": [
    "# Verbatim opgavebeskrivelse\n",
    "# Milestone 2\n",
    "DESCRIPTION\n",
    "\n",
    "After cleaning and processing our data in the first milestone, Milestone 2 will focus on how to efficiently represent the data in a database. Like last time, the milestone takes the form of a short jupyter notebook. It must be handed in on Friday, April 29, at 16:00 (in groups), and it is a requirement for attending the exam (it will be evaluated as passed/fail).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3166a8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af9342a7",
   "metadata": {},
   "source": [
    "# Task 1.\n",
    "\n",
    "The first task is to demonstrate that you have a working database containing the FakeNewsCorpus dataset. Explain your choice of schema design. You have been working on this task on a small subset of the data during the TA-sessions. For this milestone, demonstrate that your database contains a larger number of rows (e.g. one million - or however many you can reasonably work with on your available hardware), and that it supports simple queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd07546f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# psh270, jxs535, fgp424, hkp680\n",
    "import nltk as nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('corpus')\n",
    "nltk.download('all')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from cleantext.sklearn import CleanTransformer # likely required to ´pip install clean-text´\n",
    "from cleantext import clean\n",
    "from pathlib import Path\n",
    "data = pd.read_csv(r\"C:\\Users\\Computer\\Documents\\GitHub\\Data-Science-Course-Work\\CSVs\\1mio-raw.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9434dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_useless_data(data):\n",
    "    #Dropping unneeded columns\n",
    "    cols_to_delete = [\"Unnamed: 0\",\"scraped_at\",\"inserted_at\",\"updated_at\"]\n",
    "    for column in data.columns:\n",
    "        if data[column].isnull().values.all():\n",
    "            cols_to_delete.append(column)\n",
    "    data.drop(cols_to_delete, 1, inplace=True)\n",
    "    \n",
    "    #Dropping entries with nan type\n",
    "    data.dropna(subset = [\"type\"], inplace = True)\n",
    "    #Dropping entries with unknown type\n",
    "    data.drop(data.loc[data[\"type\"] == \"unknown\"].index, inplace=True)\n",
    "\n",
    "drop_useless_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55eea0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_combinations = set()\n",
    "keyword_combinations_list = [] #Ends up containing lists [combination id, keyword]\n",
    "combination_id = 0\n",
    "article_author_list = [] #Ends up containing lists [article id, author name]\n",
    "tag_list = []\n",
    "articles = []\n",
    "\n",
    "def string_to_list(s):\n",
    "    l = s.strip('][').split(', ')\n",
    "    return [w.strip(\"'\").lower() for w in l]\n",
    "\n",
    "for i, article in data.iterrows():\n",
    "    keywords = article[\"meta_keywords\"]\n",
    "    if keywords != \"['']\":\n",
    "        combination = tuple(sorted(string_to_list(keywords)))\n",
    "        \n",
    "        if combination not in keyword_combinations:\n",
    "            combination_id += 1\n",
    "            keyword_combinations.add(combination)\n",
    "            keywords_id = combination_id\n",
    "            for word in combination:\n",
    "                keyword_combinations_list.append([combination_id, word])\n",
    "    else:\n",
    "        keywords_id = np.nan\n",
    "\n",
    "    if str(article[\"authors\"]) == \"nan\":\n",
    "        article_author_list.append([article[\"id\"], np.nan])\n",
    "    else:\n",
    "        for author in string_to_list(article[\"authors\"]):\n",
    "            article_author_list.append([article[\"id\"], author])\n",
    "\n",
    "    \n",
    "    if str(article[\"tags\"]) == \"nan\":\n",
    "        tag_list.append([article[\"id\"], np.nan])\n",
    "    else:\n",
    "        for tag in string_to_list(article[\"tags\"]):\n",
    "            tag_list.append([article[\"id\"], tag])\n",
    "\n",
    "\n",
    "    articles.append([article[\"id\"], \n",
    "                    article[\"domain\"], \n",
    "                    article[\"type\"], \n",
    "                    article[\"url\"], \n",
    "                    article[\"content\"], \n",
    "                    article[\"title\"], \n",
    "                    article[\"id\"], #The article_author_list turns into a table containing article id, author name pairs\n",
    "                    keywords_id, \n",
    "                    article[\"meta_description\"],\n",
    "                    article[\"tags\"]])\n",
    "\n",
    "keyword_combinations_df = pd.DataFrame(keyword_combinations_list, columns=[\"keyword_combination_id\", \"keyword\"])\n",
    "authors_df = pd.DataFrame(article_author_list, columns=[\"article_id\", \"author_name\"])\n",
    "tags_df = pd.DataFrame(tag_list, columns=[\"article_id\", \"tag\"])\n",
    "article_df = pd.DataFrame(articles, columns=[\"article_id\", \"domain\", \"type\", \"url\", \"content\", \"title\", \"authors\", \"keyword_combination\", \"meta_description\", \"tags\"])\n",
    "\n",
    "\n",
    "keyword_combinations_path = Path(r\"C:\\Users\\Computer\\Documents\\GitHub\\Data-Science-Course-Work\\CSVs\\keyword_combinations.csv\")\n",
    "authors_path = Path(r\"C:\\Users\\Computer\\Documents\\GitHub\\Data-Science-Course-Work\\CSVs\\authors.csv\")\n",
    "tags_path = Path(r\"C:\\Users\\Computer\\Documents\\GitHub\\Data-Science-Course-Work\\CSVs\\tags.csv\")\n",
    "articles_path = Path(r\"C:\\Users\\Computer\\Documents\\GitHub\\Data-Science-Course-Work\\CSVs\\articles.csv\")\n",
    "\n",
    "keyword_combinations_df.to_csv(keyword_combinations_path, index=False)\n",
    "authors_df.to_csv(authors_path, index=False)\n",
    "tags_df.to_csv(tags_path, index=False)\n",
    "article_df.to_csv(articles_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec60f890",
   "metadata": {},
   "source": [
    "# Task 2.\n",
    "\n",
    "List the relations you have created in your database. For each relation:\n",
    "\n",
    "list its attributes\n",
    "list its functional dependencies.\n",
    "list all the primary keys.\n",
    "Is each relation in BCNF form? If not, show how to transform the tables in BCNF and explain why it might be better (or not) to use the BCNF relations in your database.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fae360",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24398f83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a3b20a0",
   "metadata": {},
   "source": [
    "# Task 3.\n",
    "\n",
    "Once your database is loaded, you can start issuing queries to better understand the characteristics of the data. Formulate the following queries in the database languages requested (in the square brackets following each item) and briefly discuss what you observe when you execute them over your database:   \n",
    "\n",
    "List the domains of news articles of reliable type and scraped at or after January 15, 2018. NOTE: Do not include duplicate domains in your answer. [Languages: relational algebra and SQL]\n",
    "List the name(s) of the most prolific author(s) of news articles of fake type. An author is among the most prolific if it has authored as many or more fake news articles as any other author in the dataset. [Languages: extended relational algebra and SQL]\n",
    "Count the pairs of article IDs that exhibit the exact same set of meta-keywords, but only return the pairs where the set of meta-keywords is not empty. [Language: SQL]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8c1689",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65270c5c",
   "metadata": {},
   "source": [
    "# Task 4.\n",
    "\n",
    "Now that we have our data in a database, let’s revisit the “interesting observations” task from Milestone 1 - but now using queries to the database. The idea is to write database queries (e.g. using GROUP BY and COUNT) that explore features of the data set that are relevant to the fake news prediction task: outliers, artefacts. It’s OK to investigate the same issues as in Milestone 1 (now using database queries) - but you are also very welcome to come up with completely new queries. You should write at least 3 such queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4232286f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03de5ca5",
   "metadata": {},
   "source": [
    "# Task 5.\n",
    "\n",
    "Just like last time, after the hand in deadline, each group will be asked to evaluate the work of three other groups, based on a short list of criteria that you can find within the peergrade system. Again, this will only work well if everyone puts some effort into providing constructive comments, so please allocate some time to do this properly. It is an opportunity to get some feedback that can help you improve your final project. The deadline for giving feedback is a week after the hand-in deadline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2700606",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35298891",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dddd9ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
