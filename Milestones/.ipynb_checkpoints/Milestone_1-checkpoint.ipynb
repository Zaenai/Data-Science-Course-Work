{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Estimated run time of notebook is 5-10 min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Error loading corpus: Package 'corpus' not found in index\n",
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pe08 is already up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\augus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n",
      "Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n"
     ]
    }
   ],
   "source": [
    "# psh270, jxs535, fgp424, hkp680\n",
    "\n",
    "# Task 2\n",
    "import nltk as nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('corpus')\n",
    "nltk.download('all')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from cleantext.sklearn import CleanTransformer # likely required to ´pip install clean-text´\n",
    "from cleantext import clean\n",
    "data = pd.read_csv(\"https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'unreliable': 6,\n",
       " 'fake': 155,\n",
       " 'clickbait': 1,\n",
       " 'conspiracy': 31,\n",
       " 'reliable': 3,\n",
       " 'bias': 6,\n",
       " 'hate': 1,\n",
       " 'junksci': 6,\n",
       " 'political': 23,\n",
       " nan: 12,\n",
       " 'unknown': 6}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type_distribution = {}\n",
    "for type in data[\"type\"]:\n",
    "    if type not in type_distribution:\n",
    "        type_distribution[type] = 1\n",
    "    else:\n",
    "        type_distribution[type] += 1\n",
    "type_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_useless_data(data):\n",
    "    #Dropping unneeded columns\n",
    "    cols_to_delete = [\"Unnamed: 0\",\"id\",\"scraped_at\",\"inserted_at\",\"updated_at\"]\n",
    "    for column in data.columns:\n",
    "        if data[column].isnull().values.all():\n",
    "            cols_to_delete.append(column)\n",
    "    data.drop(cols_to_delete, 1, inplace=True)\n",
    "    \n",
    "    #Dropping entries with nan type\n",
    "    data.dropna(subset = [\"type\"], inplace = True)\n",
    "    #Dropping entries with unknown type\n",
    "    data.drop(data.loc[data[\"type\"] == \"unknown\"].index, inplace=True)\n",
    "    not_enough_of_type = [\"clickbait\", \"reliable\", \"unreliable\", \"bias\", \"hate\", \"junksci\"]\n",
    "    for t in not_enough_of_type:\n",
    "        data.drop(data.loc[data[\"type\"] == t].index, inplace=True)\n",
    "\n",
    "drop_useless_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_numb_date = r'(<number> <number> <number>)' #YYYY/MM/DD or DD/MM/YYYY or MM/DD/YYYY\n",
    "literal_months_date= r'(jan|feb|mar|apr|may|jun|jul|aug|sep|nov|dec)\\S* ((<number> ){1,2}|([0-9]{1,2}(st|nd|rd|th)))' #Eg. jun 2nd 2020, january 23. 2021\n",
    "literal_months_reverse_date = r'((number {1,2})|[0-9]{1,2}(st|nd|rd|th)) *(jan|feb|mar|apr|may|jun|jul|aug|sep|nov|dec)\\S*' #Eg. 10th february, 4th july\n",
    "all_dates = (three_numb_date) +'|' + (literal_months_date) +'|'+ (literal_months_reverse_date)\n",
    "multiple_chars = r'(.)\\1{3,}'\n",
    "special_symbols = r'([^<>a-z ])'#Matches special symbols such as © or ™\n",
    "single_letter = r' [a-z] ' #matches single letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_type_col_distribution(col, func):\n",
    "    distribution = {}\n",
    "    for type, val in zip(data[\"type\"], data[col]):\n",
    "        if type not in distribution:\n",
    "            distribution[type] = (func(val), 1)\n",
    "        else:\n",
    "            cur_val, num_type = distribution[type]\n",
    "            distribution[type] = cur_val + func(val), num_type + 1\n",
    "    return distribution\n",
    "\n",
    "def average_type_col_distrubtion(dist):\n",
    "    avg_dist = {}\n",
    "    for key in dist:\n",
    "        total_val, num_type = dist[key]\n",
    "        avg_val = total_val / num_type\n",
    "        avg_dist[key] = avg_val\n",
    "    return avg_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fake': 61.33548387096774,\n",
       " 'conspiracy': 50.61290322580645,\n",
       " 'political': 61.65217391304348}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_len_distribution = get_type_col_distribution(\"title\", len)\n",
    "avg_title_len_distribution = average_type_col_distrubtion(title_len_distribution)\n",
    "avg_title_len_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fake': 4105.096774193548,\n",
       " 'conspiracy': 4966.580645161291,\n",
       " 'political': 3992.4347826086955}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_len_distribution = get_type_col_distribution(\"content\", len)\n",
    "avg_content_len_distribution = average_type_col_distrubtion(content_len_distribution)\n",
    "avg_content_len_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fake': 0.6709677419354839,\n",
       " 'conspiracy': 0.967741935483871,\n",
       " 'political': 0.782608695652174}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "has_author_distribution = get_type_col_distribution(\"authors\", lambda authors : 0 if str(authors) == \"nan\" else 1)\n",
    "has_author_distribution\n",
    "average_type_col_distrubtion(has_author_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_sentences(text):\n",
    "    multi_period = r'\\.{2,}'\n",
    "    #Replacing multiple periods with a single, ie. \"...\" -> \".\" \n",
    "    text = re.sub(multi_period, \".\", text)\n",
    "    count = 0\n",
    "    for s in text.split(\".\"):\n",
    "        if s != \"\":\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "num_sentences_distribution = get_type_col_distribution(\"content\", num_sentences)\n",
    "average_num_sentences_distribution = average_type_col_distrubtion(num_sentences_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(text):\n",
    "    count = 0\n",
    "    for s in text.split(\" \"):\n",
    "        if s != \"\":\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_diversity(text):\n",
    "    words = text.split(\" \")\n",
    "    num_unique_words = len(set(words))\n",
    "    return num_unique_words/len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_cleaner = CleanTransformer(fix_unicode=True,               # fix various unicode errors\n",
    "                                    to_ascii=True,                  # transliterate to closest ASCII representation\n",
    "                                    lower=True,                     # lowercase text\n",
    "                                    no_line_breaks=True,           # fully strip line breaks as opposed to only normalizing them\n",
    "                                    no_urls=True,                  # replace all URLs with a special token\n",
    "                                    no_emails=True,                # replace all email addresses with a special token\n",
    "                                    no_phone_numbers=True,         # replace all phone numbers with a special token\n",
    "                                    no_numbers=False,               # replace all numbers with a special token\n",
    "                                    no_digits=False,                # replace all digits with a special token\n",
    "                                    no_currency_symbols=True,      # replace all currency symbols with a special token\n",
    "                                    no_punct=True,                 # remove punctuations\n",
    "                                    replace_with_punct=\" \",          # instead of removing punctuations you may replace them\n",
    "                                    replace_with_url=\"<url>\",\n",
    "                                    replace_with_email=\"<email>\",\n",
    "                                    replace_with_phone_number=\"<phone>\",\n",
    "                                    replace_with_currency_symbol=\"<cur>\",\n",
    "                                    lang=\"en\"                       # set to 'de' for German special handling\n",
    "                                    )\n",
    "\n",
    "general_cleaner = CleanTransformer(fix_unicode=False,               # fix various unicode errors\n",
    "                                    to_ascii=False,                  # transliterate to closest ASCII representation\n",
    "                                    lower=False,                     # lowercase text\n",
    "                                    no_line_breaks=False,           # fully strip line breaks as opposed to only normalizing them\n",
    "                                    no_urls=False,                  # replace all URLs with a special token\n",
    "                                    no_emails=False,                # replace all email addresses with a special token\n",
    "                                    no_phone_numbers=False,         # replace all phone numbers with a special token\n",
    "                                    no_numbers=True,               # replace all numbers with a special token\n",
    "                                    no_digits=False,                # replace all digits with a special token\n",
    "                                    no_currency_symbols=False,      # replace all currency symbols with a special token\n",
    "                                    no_punct=False,                 # remove punctuations\n",
    "                                    replace_with_number=\"<number>\",\n",
    "                                    lang=\"en\"                       # set to 'de' for German special handling\n",
    "                                    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch tags, compatibility with lemmatise() \n",
    "def switchTag(tag):\n",
    "    if tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif (tag.startswith('J') or\n",
    "            tag.startswith('A')):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN \n",
    "\n",
    "#string_test='In gold, the open interest SURPRISINGLY ROSE BY A CONSIDERABLE 9126 CONTRACTS UP TO582,421 WITH THE GOOD SIZED RISE IN PRICE OF GOLD WITH YESTERDAY’S TRADING ($5.55). IN ANOTHER HUGE DEVELOPMENT, WE RECEIVED THE TOTAL NUMBER OF GOLD EFP’S ISSUED FOR WEDNESDAY AND IT TOTALED A HUMONGOUS SIZED 12,223 CONTRACTS OF WHICH FEBRUARY SAW 11,023 CONTRACTS ISSUED AND APRIL SAW THE ISSUANCE OF 1200 CONTRACTS.'\n",
    "#date_test  = '12/18/10 12/18/2020 12-18-10 12-18-2020 12/18/10 12/18/2020 12.18.10 12.18.2020 noise 12182010 december 18, 2010 janu 10th march 1st 3st january Dekjkj 10th  noise 10/20  noise noise 2020 10th january 2021'\n",
    "\n",
    "def clean_column(data, col_name):\n",
    "    for i, entry in zip(data[col_name].index, data[col_name]):    \n",
    "        #We first convert to lower case and replace punctuation with space such that dates can\n",
    "        #more easily be processed (eg. 10.12.2020 -> 10 12 2020 -> <NUMBER> <NUMBER> <NUMBER> instead of <NUMBER><NUMBER><DIGIT> or something)\n",
    "        cleaned = initial_cleaner.transform([entry])[0]\n",
    "        cleaned = general_cleaner.transform([cleaned])[0]\n",
    "        cleaned = re.sub(all_dates, '<date> ', cleaned)\n",
    "        cleaned = re.sub(special_symbols,'',cleaned)\n",
    "        cleaned = re.sub(multiple_chars, '', cleaned)\n",
    "        cleaned = re.sub(single_letter, '',cleaned)\n",
    "        data.at[i, col_name] = cleaned\n",
    "\n",
    "def clean_data(data):\n",
    "    clean_column(data, \"content\")\n",
    "    clean_column(data, \"title\")\n",
    "\n",
    "\n",
    "def lemmatise_text(text): \n",
    "    buff = word_tokenize(text)\n",
    "    buff = nltk.pos_tag(buff)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatised = []\n",
    "    for index, word in enumerate(buff):\n",
    "        buff[index] = (word[0], switchTag(word[1]))\n",
    "        lemmatised.append(lemmatizer.lemmatize(buff[index][0],pos=buff[index][1]))\n",
    "    return lemmatised\n",
    "\n",
    "def count_and_clean(data, col_name):\n",
    "    retVal = []\n",
    "    last_length = 0\n",
    "    tags = [\"<number>\",\"<date>\",\"<url>\",\"<email>\",\"<phone>\",\"<email>\"]\n",
    "    for i, entry in zip(data[col_name].index, data[col_name]):\n",
    "        cleaned = entry\n",
    "        arr = []\n",
    "        last_length = len(entry)\n",
    "        for tag in tags: \n",
    "            cleaned = cleaned.replace(tag, \"\")\n",
    "            data.at[i, col_name] = cleaned\n",
    "            arr.append((last_length-len(cleaned),tag))\n",
    "            last_length = len(cleaned)\n",
    "        retVal.append(arr)\n",
    "    return retVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning\n",
    "clean_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fake': 0.5760302749007344,\n",
       " 'conspiracy': 0.5625997901851042,\n",
       " 'political': 0.5915298531509602}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean tags <..>\n",
    "count = count_and_clean(data, \"content\")\n",
    "\n",
    "# CALL WORD COUNT\n",
    "\n",
    "lexical_diversity_distribution = get_type_col_distribution(\"content\", lexical_diversity)\n",
    "average_lexical_diversity = average_type_col_distrubtion(lexical_diversity_distribution)\n",
    "average_lexical_diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fake': 6.747078658834009,\n",
       " 'conspiracy': 6.531370635897002,\n",
       " 'political': 6.581565366972478}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_distribution = get_type_col_distribution(\"content\", word_count)\n",
    "average_word_count_distribution = average_type_col_distrubtion(word_count_distribution)\n",
    "average_word_count_distribution\n",
    "average_word_len_distrubution = {t: avg_content_len_distribution[t]/average_word_count_distribution[t] for t in average_word_count_distribution}\n",
    "average_word_len_distrubution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fake': 16.435343325200417,\n",
       " 'conspiracy': 19.211898940505296,\n",
       " 'political': 16.729016786570742}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_sentence_len_distribution = {t: average_word_count_distribution[t]/average_num_sentences_distribution[t] for t in average_word_count_distribution}\n",
    "average_sentence_len_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_complete_text_distribution(col):\n",
    "    distribution = {}\n",
    "    for type, text in zip(data[\"type\"], data[col]):\n",
    "        if type not in distribution:\n",
    "            distribution[type] = text\n",
    "        else:\n",
    "            distribution[type] += \" \" + text\n",
    "    return distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing before finding frequency distribution:\n",
    "eng_stopwords = stopwords.words('english')\n",
    "#Find the frequency distribution for every type, eg. frequency distribution when looking at all fake news articles concatenated \n",
    "text_distribution = get_complete_text_distribution(\"content\")\n",
    "\n",
    "for type in text_distribution:\n",
    "    # lematise \n",
    "    text_distribution[type] = lemmatise_text(text_distribution[type])\n",
    "    # Delete stop-words\n",
    "    text_distribution[type] = [word for word in text_distribution[type] if word not in eng_stopwords]\n",
    "    # create freqDist object \n",
    "    text_distribution[type] = FreqDist(text_distribution[type])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fake:\n",
      "FreqDist({'blockchain': 260, 'market': 258, 'one': 244, 'time': 222, 'trump': 214, 'state': 203, 'say': 199, 'people': 195, 'like': 190, 'year': 188, ...})\n",
      "conspiracy:\n",
      "FreqDist({'one': 91, 'government': 75, 'year': 67, 'time': 60, 'american': 55, 'make': 49, 'state': 47, 'use': 46, 'obama': 46, 'people': 45, ...})\n",
      "political:\n",
      "FreqDist({'say': 70, 'people': 66, 'go': 52, 'state': 51, 'one': 46, 'thing': 43, 'would': 34, 'take': 32, 'use': 32, 'american': 31, ...})\n"
     ]
    }
   ],
   "source": [
    "for type in text_distribution:\n",
    "    print(type + \":\")\n",
    "    text_distribution[type].pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3\n",
    "We have the following number of articles of each type:\n",
    "fake: 155\n",
    "conspiracy: 31\n",
    "political: 23\n",
    "unreliable: 6\n",
    "junksci: 6\n",
    "bias: 6\n",
    "reliable: 3\n",
    "clickbait: 1\n",
    "hate: 1\n",
    "\n",
    "From this we can see that there are not enough representants of classes 'hate', 'clickbait', 'reliable', 'bias', 'junksci', 'unreliable' to make meaningful observations. \n",
    "Therefore we chose to not include the listed types in our analysis.\n",
    "\n",
    "We have observed that conspiracy articles often have authors noted (0.96%) compared to fake news (0.67%) and political (78%). \n",
    "\n",
    "We have observed that conspiracy articles tend to be longer.\n",
    "\n",
    "We have observed that words such as \"blockchain\", \"market\", \"trump\", and \"state\" are relatively common in fake news. \n",
    "For conspiracy common words of interest instead include \"government\", \"american\", and \"obama\".\n",
    "And for political these include \"state\" and \"american\".\n",
    "\n",
    "We have observed that conspiracy tends to use longer sentences than fake and political (19.2 words on average compared to 16.4 and 16.73 respectively).\n",
    "\n",
    "We have observed that all fake news articles have been distributed by beforeitsnews.com.\n",
    "\n",
    "Besides that we have observed that no fake news articles contains meta_keywords or meta_description.\n",
    "\n",
    "Reliable data seems to have lower word length count per sentence and longer sentence length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASK 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AOPRSTUVWZ\n"
     ]
    }
   ],
   "source": [
    "# Initialize Group SubString\n",
    "group_nr = 14\n",
    "group_substring_raw = \"ABCDEFGHIJKLMNOPRSTUVWZABCDEFGHIJKLMNOPRSTUVWZ\"[group_nr%23:group_nr%23+10]\n",
    "group_substring = \"\"\n",
    "\n",
    "for letter in np.sort(list(group_substring_raw)):\n",
    "    group_substring += letter\n",
    "    \n",
    "print(group_substring)\n",
    "#group_substring = \"Z\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get main page and add subpage (according to group_substring) urls to list\n",
    "response = requests.get('https://en.wikinews.org/wiki/Category:Politics_and_conflicts')\n",
    "contents = response.text\n",
    "\n",
    "soup = BeautifulSoup(contents, 'html.parser')\n",
    "\n",
    "subpages = []\n",
    "for a in soup.find_all('a', href=True):\n",
    "    for letter in group_substring:\n",
    "        if \"conflicts&from=\"+letter in a[\"href\"]:\n",
    "            if not a[\"href\"] in subpages:\n",
    "                subpages.append(a[\"href\"])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_subpages = []\n",
    "for subpage in subpages:\n",
    "    is_digging = True\n",
    "    new_subpages = []\n",
    "\n",
    "    while is_digging:\n",
    "        curLetter = str(re.search(\"from=.\",subpage))[-3]\n",
    "        next_page = None\n",
    "\n",
    "        response = requests.get(subpage)\n",
    "        contents = response.text\n",
    "        soup = BeautifulSoup(contents, 'html.parser')\n",
    "        page_cands = soup.find_all('a', href=True)\n",
    "\n",
    "        for n in page_cands:\n",
    "            if \"next page\" in str(n):\n",
    "                next_page = n\n",
    "                if str(next_page[\"href\"])[60] == curLetter:\n",
    "                    new_subpages.append(\"https://en.wikinews.org/\"+next_page[\"href\"])\n",
    "                else:\n",
    "                    is_digging = False\n",
    "                subpage = \"https://en.wikinews.org/\"+next_page[\"href\"]\n",
    "                break\n",
    "        if not next_page:\n",
    "            break\n",
    "\n",
    "    for sp in new_subpages:\n",
    "        expanded_subpages.append(sp)\n",
    "    \n",
    "for esp in expanded_subpages:\n",
    "    subpages.append(esp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to show all pages that contain scraped articles\n",
    "# subpages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a list of article urls to scrape later\n",
    "Articles = []\n",
    "for url in subpages:\n",
    "    response = requests.get(url)\n",
    "    contents = response.text\n",
    "\n",
    "    soup = BeautifulSoup(contents, 'html.parser')\n",
    "\n",
    "    curLetter = str(re.search(\"from=.\",url))[-3]\n",
    "    allGroups = soup.find_all(\"div\",attrs={\"class\":\"mw-category-group\"})\n",
    "    for n in allGroups:\n",
    "        if \"<h3>\"+curLetter+\"</h3>\" in str(n) and \"<ul><li><a\" in str(n):\n",
    "            pages = n\n",
    "            break\n",
    "    ul = re.findall('\\/wiki.*(?=title)',str(pages))\n",
    "    for i in range(len(ul)):\n",
    "        ul[i] = \"https://en.wikinews.org\" + ul[i][:-2]\n",
    "\n",
    "    Articles.append(ul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tags(text):\n",
    "    TAG_RE = re.compile(r'<[^>]+>')\n",
    "    return TAG_RE.sub('', text)\n",
    "\n",
    "def GA_GetDate(soup):\n",
    "    try: # Need to convert from \"MonthName Day, Year\" to \"Year-Month-Day\"\n",
    "        date = re.findall('[1-9]+.*[1-9]',str(soup.find(\"span\", attrs={\"id\":\"publishDate\"})))[0]\n",
    "    except:\n",
    "        try: \n",
    "            date = re.findall('[A-Z][a-z]+ [0-9]+, [0-9][0-9][0-9][0-9]',str(soup.find(\"div\", attrs={\"class\":\"mw-parser-output\"})))[0]\n",
    "        except:\n",
    "            date = \"NaN\"\n",
    "    return date\n",
    "\n",
    "def GA_GetText(soup):\n",
    "    try:\n",
    "        text = soup.get_text() #currently displays EVERYTHING on page, needs work\n",
    "    except:\n",
    "        text = \"NaN\"\n",
    "    return text\n",
    "        \n",
    "def GA_GetSources(soup):\n",
    "    srcs = []\n",
    "    try:\n",
    "        src = soup.find_all(\"span\",attrs={\"class\":\"sourceTemplate\"})\n",
    "        for n in src:\n",
    "            m = remove_tags(str(n))\n",
    "            srcs.append(m)      \n",
    "    except:\n",
    "        src = \"NaN\"\n",
    "    return srcs\n",
    "        \n",
    "def GA_GetTitle(soup):\n",
    "    try: \n",
    "        title = soup.find(\"h1\",attrs={\"id\":\"firstHeading\"})\n",
    "        title = remove_tags(str(title))\n",
    "    except:\n",
    "        title = \"NaN\"\n",
    "    return title\n",
    "        \n",
    "def GA_GetContent(soup):\n",
    "    try:\n",
    "        article_text = \"\"\n",
    "        article = soup.find(\"div\",attrs={\"class\":\"mw-parser-output\"}).findAll('p')\n",
    "        for element in article:\n",
    "            article_text += '\\n' + ''.join(element.findAll(text = True))\n",
    "        test_text = article_text.replace(\"\\n\",\"\")\n",
    "        if test_text == \"\":\n",
    "            test_text = re.findall('^.*(?=Have an opinion on this story?)',article_text)\n",
    "            if test_text == \"\":\n",
    "                article_text = \"NaN\"\n",
    "            else:\n",
    "                article_text = test_text\n",
    "                article_text = re.sub('[A-Z][a-z]*, [A-Z][a-z]* [0-9]*, [0-9]*',\"\",article_text)\n",
    "                article_text = re.sub('File:.*\\.[a-z][a-z][a-z][a-z]?(?=)',\"\",article_text)\n",
    "        else:\n",
    "            article_text = test_text\n",
    "            article_text = re.sub('[A-Z][a-z]*, [A-Z][a-z]* [0-9]*, [0-9]*',\"\",article_text)\n",
    "            article_text = re.sub('File:.*\\.[a-z][a-z][a-z][a-z]?(?=)',\"\",article_text)\n",
    "            \n",
    "    except:\n",
    "        article_text = \"NaN\"\n",
    "    return article_text\n",
    "        \n",
    "def GA_GetCategories(soup):\n",
    "    try:\n",
    "        categories = []\n",
    "        cat = soup.find(\"div\",attrs={\"class\":\"mw-normal-catlinks\"})\n",
    "        cat = cat.findAll(\"ul\")[0]\n",
    "        for c in cat:\n",
    "            cat = remove_tags(str(c))\n",
    "            categories.append(cat)\n",
    "    except:\n",
    "        categories = \"NaN\"\n",
    "    return categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GrabArticle(url):\n",
    "    \n",
    "    # init soup stuff\n",
    "    response = requests.get(url)\n",
    "    contents = response.text\n",
    "    soup = BeautifulSoup(contents, 'html.parser')\n",
    "    \n",
    "    # Will try to find date, date, sources, title, text and categories, if none found/error, will return NaN\n",
    "    date = GA_GetDate(soup)        \n",
    "    text = GA_GetText(soup)\n",
    "    srcs = GA_GetSources(soup)      \n",
    "    title = GA_GetTitle(soup)\n",
    "    article_text = GA_GetContent(soup) # Also removes date formatting from top of page\n",
    "    categories = GA_GetCategories(soup)\n",
    "\n",
    "    words = article_text.split()\n",
    "    avg_word = sum(len(word) for word in words) / len(words)\n",
    "    \n",
    "\n",
    "    return date, srcs, title, article_text, categories, avg_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>(Raw) No. Words</th>\n",
       "      <th>(Raw) Avg. Word Length</th>\n",
       "      <th>Date written</th>\n",
       "      <th>Content</th>\n",
       "      <th>Categories</th>\n",
       "      <th>URL</th>\n",
       "      <th>Sources</th>\n",
       "      <th>Scraped at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A 1-year long strike against FMC Novamed: Wome...</td>\n",
       "      <td>674</td>\n",
       "      <td>5.081602</td>\n",
       "      <td>2007-09-17</td>\n",
       "      <td>In a free trade zone in Antalya, Turkey, 80 w...</td>\n",
       "      <td>[September 17, 2007, Articles with broken sour...</td>\n",
       "      <td>https://en.wikinews.org/wiki/A_1-year_long_str...</td>\n",
       "      <td>[Ertuğrul Mavioğlu. \"Serbest bölgede bir yıllı...</td>\n",
       "      <td>06/06/2022 16:14:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A policeman is killed and another one is tortu...</td>\n",
       "      <td>578</td>\n",
       "      <td>5.399654</td>\n",
       "      <td>2005-02-16</td>\n",
       "      <td>On  the police officer Luiz Pereira da Silva ...</td>\n",
       "      <td>[February 16, 2005, Published, Archived, Brazi...</td>\n",
       "      <td>https://en.wikinews.org/wiki/A_policeman_is_ki...</td>\n",
       "      <td>[Késia Souza. \"SDS reforçará investigação\" — F...</td>\n",
       "      <td>06/06/2022 16:14:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A timeline: Novak, Rove, Cooper</td>\n",
       "      <td>299</td>\n",
       "      <td>5.113712</td>\n",
       "      <td>2005-07-15</td>\n",
       "      <td>The Novak story that sparked the contoversy w...</td>\n",
       "      <td>[July 15, 2005, Iraq War, Iraq, Politics and c...</td>\n",
       "      <td>https://en.wikinews.org/wiki/A_timeline:_Novak...</td>\n",
       "      <td>[David Johnston and Richard W. Stevenson. \"Sou...</td>\n",
       "      <td>06/06/2022 16:14:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abbas and Olmert meet before Bush visit</td>\n",
       "      <td>471</td>\n",
       "      <td>5.154989</td>\n",
       "      <td>2008-01-08</td>\n",
       "      <td>Israeli Prime Minister Ehud Olmert and Palest...</td>\n",
       "      <td>[January 8, 2008, Published, Archived, Middle ...</td>\n",
       "      <td>https://en.wikinews.org/wiki/Abbas_and_Olmert_...</td>\n",
       "      <td>[Jim Teeple. \"Abbas, Olmert Meet Before Bush V...</td>\n",
       "      <td>06/06/2022 16:14:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Abbas fires security chiefs for failure to cur...</td>\n",
       "      <td>245</td>\n",
       "      <td>5.306122</td>\n",
       "      <td>2005-04-02</td>\n",
       "      <td>Palestinian Authority chairman Mahmoud Abbas ...</td>\n",
       "      <td>[April 2, 2005, Published, Archived, Palestine...</td>\n",
       "      <td>https://en.wikinews.org/wiki/Abbas_fires_secur...</td>\n",
       "      <td>[Mohammed Assadi. \"Abbas Fires Security Chiefs...</td>\n",
       "      <td>06/06/2022 16:14:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3665</th>\n",
       "      <td>Write-in candidate leads in Buffalo, New York ...</td>\n",
       "      <td>516</td>\n",
       "      <td>5.310078</td>\n",
       "      <td>2021-11-04</td>\n",
       "      <td>In a mayoral election held Tuesday in Buffalo...</td>\n",
       "      <td>[November 4, 2021, North America, United State...</td>\n",
       "      <td>https://en.wikinews.org/wiki/Write-in_candidat...</td>\n",
       "      <td>[Associated Press. \"Buffalo Mayor Claims Write...</td>\n",
       "      <td>06/06/2022 16:22:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3666</th>\n",
       "      <td>Writers Guild of America ends strike</td>\n",
       "      <td>538</td>\n",
       "      <td>5.048327</td>\n",
       "      <td>2008-02-13</td>\n",
       "      <td>Hollywood writers have voted to end a three-m...</td>\n",
       "      <td>[February 13, 2008, Culture and entertainment,...</td>\n",
       "      <td>https://en.wikinews.org/wiki/Writers_Guild_of_...</td>\n",
       "      <td>[Mike O'Sullivan. \"Hollywood Writers End Three...</td>\n",
       "      <td>06/06/2022 16:22:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3667</th>\n",
       "      <td>Writs issued for 2012 Queensland election</td>\n",
       "      <td>504</td>\n",
       "      <td>5.392857</td>\n",
       "      <td>2012-02-22</td>\n",
       "      <td>The 2012 Queensland election has been officia...</td>\n",
       "      <td>[February 22, 2012, Published, Archived, Polit...</td>\n",
       "      <td>https://en.wikinews.org/wiki/Writs_issued_for_...</td>\n",
       "      <td>[ \"Member list - Queensland Parliament\" — Quee...</td>\n",
       "      <td>06/06/2022 16:22:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3668</th>\n",
       "      <td>Wrong flag causes diplomatic faux pas in Poland</td>\n",
       "      <td>257</td>\n",
       "      <td>5.350195</td>\n",
       "      <td>2006-10-06</td>\n",
       "      <td>During the official visit of Sergey Lavrov, F...</td>\n",
       "      <td>[October 6, 2006, Published, Archived, Europe,...</td>\n",
       "      <td>https://en.wikinews.org/wiki/Wrong_flag_causes...</td>\n",
       "      <td>[ \"Ławrow w Warszawie. Miłe spotkanie bez konk...</td>\n",
       "      <td>06/06/2022 16:22:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3669</th>\n",
       "      <td>Wyclef Jean considering standing for president...</td>\n",
       "      <td>564</td>\n",
       "      <td>5.280142</td>\n",
       "      <td>2010-07-28</td>\n",
       "      <td>Following months of rumours, musician Wyclef ...</td>\n",
       "      <td>[July 28, 2010, United States, Politics and co...</td>\n",
       "      <td>https://en.wikinews.org/wiki/Wyclef_Jean_consi...</td>\n",
       "      <td>[Rory Carroll. \"Wyclef Jean eyes Haiti preside...</td>\n",
       "      <td>06/06/2022 16:22:20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3670 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Title  (Raw) No. Words  \\\n",
       "0     A 1-year long strike against FMC Novamed: Wome...              674   \n",
       "1     A policeman is killed and another one is tortu...              578   \n",
       "2                       A timeline: Novak, Rove, Cooper              299   \n",
       "3               Abbas and Olmert meet before Bush visit              471   \n",
       "4     Abbas fires security chiefs for failure to cur...              245   \n",
       "...                                                 ...              ...   \n",
       "3665  Write-in candidate leads in Buffalo, New York ...              516   \n",
       "3666               Writers Guild of America ends strike              538   \n",
       "3667          Writs issued for 2012 Queensland election              504   \n",
       "3668    Wrong flag causes diplomatic faux pas in Poland              257   \n",
       "3669  Wyclef Jean considering standing for president...              564   \n",
       "\n",
       "      (Raw) Avg. Word Length Date written  \\\n",
       "0                   5.081602   2007-09-17   \n",
       "1                   5.399654   2005-02-16   \n",
       "2                   5.113712   2005-07-15   \n",
       "3                   5.154989   2008-01-08   \n",
       "4                   5.306122   2005-04-02   \n",
       "...                      ...          ...   \n",
       "3665                5.310078   2021-11-04   \n",
       "3666                5.048327   2008-02-13   \n",
       "3667                5.392857   2012-02-22   \n",
       "3668                5.350195   2006-10-06   \n",
       "3669                5.280142   2010-07-28   \n",
       "\n",
       "                                                Content  \\\n",
       "0      In a free trade zone in Antalya, Turkey, 80 w...   \n",
       "1      On  the police officer Luiz Pereira da Silva ...   \n",
       "2      The Novak story that sparked the contoversy w...   \n",
       "3      Israeli Prime Minister Ehud Olmert and Palest...   \n",
       "4      Palestinian Authority chairman Mahmoud Abbas ...   \n",
       "...                                                 ...   \n",
       "3665   In a mayoral election held Tuesday in Buffalo...   \n",
       "3666   Hollywood writers have voted to end a three-m...   \n",
       "3667   The 2012 Queensland election has been officia...   \n",
       "3668   During the official visit of Sergey Lavrov, F...   \n",
       "3669   Following months of rumours, musician Wyclef ...   \n",
       "\n",
       "                                             Categories  \\\n",
       "0     [September 17, 2007, Articles with broken sour...   \n",
       "1     [February 16, 2005, Published, Archived, Brazi...   \n",
       "2     [July 15, 2005, Iraq War, Iraq, Politics and c...   \n",
       "3     [January 8, 2008, Published, Archived, Middle ...   \n",
       "4     [April 2, 2005, Published, Archived, Palestine...   \n",
       "...                                                 ...   \n",
       "3665  [November 4, 2021, North America, United State...   \n",
       "3666  [February 13, 2008, Culture and entertainment,...   \n",
       "3667  [February 22, 2012, Published, Archived, Polit...   \n",
       "3668  [October 6, 2006, Published, Archived, Europe,...   \n",
       "3669  [July 28, 2010, United States, Politics and co...   \n",
       "\n",
       "                                                    URL  \\\n",
       "0     https://en.wikinews.org/wiki/A_1-year_long_str...   \n",
       "1     https://en.wikinews.org/wiki/A_policeman_is_ki...   \n",
       "2     https://en.wikinews.org/wiki/A_timeline:_Novak...   \n",
       "3     https://en.wikinews.org/wiki/Abbas_and_Olmert_...   \n",
       "4     https://en.wikinews.org/wiki/Abbas_fires_secur...   \n",
       "...                                                 ...   \n",
       "3665  https://en.wikinews.org/wiki/Write-in_candidat...   \n",
       "3666  https://en.wikinews.org/wiki/Writers_Guild_of_...   \n",
       "3667  https://en.wikinews.org/wiki/Writs_issued_for_...   \n",
       "3668  https://en.wikinews.org/wiki/Wrong_flag_causes...   \n",
       "3669  https://en.wikinews.org/wiki/Wyclef_Jean_consi...   \n",
       "\n",
       "                                                Sources           Scraped at  \n",
       "0     [Ertuğrul Mavioğlu. \"Serbest bölgede bir yıllı...  06/06/2022 16:14:45  \n",
       "1     [Késia Souza. \"SDS reforçará investigação\" — F...  06/06/2022 16:14:45  \n",
       "2     [David Johnston and Richard W. Stevenson. \"Sou...  06/06/2022 16:14:45  \n",
       "3     [Jim Teeple. \"Abbas, Olmert Meet Before Bush V...  06/06/2022 16:14:46  \n",
       "4     [Mohammed Assadi. \"Abbas Fires Security Chiefs...  06/06/2022 16:14:46  \n",
       "...                                                 ...                  ...  \n",
       "3665  [Associated Press. \"Buffalo Mayor Claims Write...  06/06/2022 16:22:19  \n",
       "3666  [Mike O'Sullivan. \"Hollywood Writers End Three...  06/06/2022 16:22:19  \n",
       "3667  [ \"Member list - Queensland Parliament\" — Quee...  06/06/2022 16:22:19  \n",
       "3668  [ \"Ławrow w Warszawie. Miłe spotkanie bez konk...  06/06/2022 16:22:19  \n",
       "3669  [Rory Carroll. \"Wyclef Jean eyes Haiti preside...  06/06/2022 16:22:20  \n",
       "\n",
       "[3670 rows x 9 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scraping article pages for data and adding to lists for dataframe creation\n",
    "urls,dates,sources,titles,article_text,scraped_at,numberOfWords,categories,avg_words = [],[],[],[],[],[],[],[],[]\n",
    "\n",
    "for articles in Articles:\n",
    "    for url in articles:\n",
    "        now = datetime.now()\n",
    "        \n",
    "        d,s,t,at,c,aw = GrabArticle(url)\n",
    "        \n",
    "        urls.append(url)\n",
    "        \n",
    "        dates.append(d)\n",
    "        sources.append(s)\n",
    "        titles.append(t)\n",
    "        article_text.append(at)\n",
    "        categories.append(c)\n",
    "        \n",
    "        avg_words.append(aw)\n",
    "        \n",
    "        \n",
    "        numberOfWords.append(len(at.split()))\n",
    "        \n",
    "        \n",
    "        scraped_at.append(now.strftime(\"%d/%m/%Y %H:%M:%S\"))\n",
    "        \n",
    "        \n",
    "        \n",
    "Task4df = pd.DataFrame(data = {\"Title\" : titles,  \"(Raw) No. Words\" : numberOfWords, \"(Raw) Avg. Word Length\" : avg_words, \"Date written\" : dates, \"Content\": article_text, \"Categories\" : categories , \"URL\" : urls, \"Sources\" : sources, \"Scraped at\" : scraped_at})\n",
    "\n",
    "Task4df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the following libraries:\\\n",
    "requests, beautifulsoup (bs4), regular expressions (re).\n",
    "\n",
    "Key issues:\\\n",
    "    - Slow scraping speeds (1700+ articles)\\\n",
    "    - Inconsistent HTML tags and layouts\\\n",
    "    - Inconsistent text formatting (e.g. Dates are written as both November 11, 1111 and 1111-11-11)\\\n",
    "    - The end of articles do not terminate the same, they are padded with wikipedia metadata of indeterminate width and content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "78f06ca9406df9cd700ff8c04aca5c1ebc2f7c29a6cfb7725741d9325210d4ad"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
