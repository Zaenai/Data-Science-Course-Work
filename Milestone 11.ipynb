{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# psh270, jxs535, fgp424, hkp680\n",
    "\n",
    "# Task 2\n",
    "import nltk as nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('corpus')\n",
    "nltk.download('all')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from cleantext.sklearn import CleanTransformer # likely required to ´pip install clean-text´\n",
    "data = pd.read_csv(\"https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_useless_data(data):\n",
    "    #Dropping unneeded columns\n",
    "    cols_to_delete = [\"Unnamed: 0\",\"id\",\"scraped_at\",\"inserted_at\",\"updated_at\"]\n",
    "    for column in data.columns:\n",
    "        if data[column].isnull().values.all():\n",
    "            cols_to_delete.append(column)\n",
    "    data.drop(cols_to_delete, 1, inplace=True)\n",
    "    \n",
    "    #Dropping entries with nan type\n",
    "    data.dropna(subset = [\"type\"], inplace = True)\n",
    "    #Dropping entries with unknown type\n",
    "    data.drop(data.loc[data[\"type\"] == \"unknown\"].index, inplace=True)\n",
    "\n",
    "drop_useless_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_distribution = {}\n",
    "for type in data[\"type\"]:\n",
    "    if type not in type_distribution:\n",
    "        type_distribution[type] = 1\n",
    "    else:\n",
    "        type_distribution[type] += 1\n",
    "type_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = type_distribution[\"fake\"]\n",
    "other = 0\n",
    "for type in type_distribution:\n",
    "    if type != \"fake\":\n",
    "        other += type_distribution[type]\n",
    "\n",
    "print(f\"fake: {fake}, other: {other}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_type_col_distribution(col, func):\n",
    "    distribution = {}\n",
    "    for type, val in zip(data[\"type\"], data[col]):\n",
    "        if type not in distribution:\n",
    "            distribution[type] = (func(val), 1)\n",
    "        else:\n",
    "            cur_val, num_type = distribution[type]\n",
    "            distribution[type] = cur_val + func(val), num_type + 1\n",
    "    return distribution\n",
    "\n",
    "def average_type_col_distrubtion(dist):\n",
    "    avg_dist = {}\n",
    "    for key in dist:\n",
    "        total_val, num_type = dist[key]\n",
    "        avg_val = total_val / num_type\n",
    "        avg_dist[key] = avg_val\n",
    "    return avg_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_len_distribution = get_type_col_distribution(\"title\", len)\n",
    "avg_title_len_distribution = average_type_col_distrubtion(title_len_distribution)\n",
    "avg_title_len_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_len_distribution = get_type_col_distribution(\"content\", len)\n",
    "avg_content_len_distribution = average_type_col_distrubtion(content_len_distribution)\n",
    "avg_content_len_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_author_distribution = get_type_col_distribution(\"authors\", lambda authors : 0 if str(authors) == \"nan\" else 1)\n",
    "has_author_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_cleaner = CleanTransformer(fix_unicode=True,               # fix various unicode errors\n",
    "                                    to_ascii=True,                  # transliterate to closest ASCII representation\n",
    "                                    lower=True,                     # lowercase text\n",
    "                                    no_line_breaks=True,           # fully strip line breaks as opposed to only normalizing them\n",
    "                                    no_urls=True,                  # replace all URLs with a special token\n",
    "                                    no_emails=True,                # replace all email addresses with a special token\n",
    "                                    no_phone_numbers=True,         # replace all phone numbers with a special token\n",
    "                                    no_numbers=False,               # replace all numbers with a special token\n",
    "                                    no_digits=False,                # replace all digits with a special token\n",
    "                                    no_currency_symbols=True,      # replace all currency symbols with a special token\n",
    "                                    no_punct=True,                 # remove punctuations\n",
    "                                    replace_with_punct=\" \",          # instead of removing punctuations you may replace them\n",
    "                                    replace_with_url=\"<url>\",\n",
    "                                    replace_with_email=\"<email>\",\n",
    "                                    replace_with_phone_number=\"<phone>\",\n",
    "                                    replace_with_currency_symbol=\"<cur>\",\n",
    "                                    lang=\"en\"                       # set to 'de' for German special handling\n",
    "                                    )\n",
    "\n",
    "general_cleaner = CleanTransformer(fix_unicode=False,               # fix various unicode errors\n",
    "                                    to_ascii=False,                  # transliterate to closest ASCII representation\n",
    "                                    lower=False,                     # lowercase text\n",
    "                                    no_line_breaks=False,           # fully strip line breaks as opposed to only normalizing them\n",
    "                                    no_urls=False,                  # replace all URLs with a special token\n",
    "                                    no_emails=False,                # replace all email addresses with a special token\n",
    "                                    no_phone_numbers=False,         # replace all phone numbers with a special token\n",
    "                                    no_numbers=True,               # replace all numbers with a special token\n",
    "                                    no_digits=False,                # replace all digits with a special token\n",
    "                                    no_currency_symbols=False,      # replace all currency symbols with a special token\n",
    "                                    no_punct=False,                 # remove punctuations\n",
    "                                    replace_with_number=\"<number>\",\n",
    "                                    lang=\"en\"                       # set to 'de' for German special handling\n",
    "                                    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WordFreq(col_name, article_number, input):\n",
    "    q = input[col_name][article_number]\n",
    "    unique_words = set(q)\n",
    "    unique_word_count = len(unique_words)\n",
    "    qqq = len(q)/unique_word_count\n",
    "    return qqq\n",
    "\n",
    "def WordFreqSet(set_value , set_data):\n",
    "    WordFreqArray = []\n",
    "    for col in set_data.index:\n",
    "        WordFreqArray.append(WordFreq(set_value, col, set_data))\n",
    "    return WordFreqArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_numb_date = r'(<number> <number> <number>)' #YYYY/MM/DD or DD/MM/YYYY or MM/DD/YYYY\n",
    "literal_months_date= r'(jan|feb|mar|apr|may|jun|jul|aug|sep|nov|dec)\\S* ((<number> ){1,2}|([0-9]{1,2}(st|nd|rd|th)))' #Eg. jun 2nd 2020, january 23. 2021\n",
    "literal_months_reverse_date = r'((number {1,2})|[0-9]{1,2}(st|nd|rd|th)) *(jan|feb|mar|apr|may|jun|jul|aug|sep|nov|dec)\\S*' #Eg. 10th february, 4th july\n",
    "all_dates = (three_numb_date) +'|' + (literal_months_date) +'|'+ (literal_months_reverse_date)\n",
    "multiple_chars = r'(.)\\1{3,}'\n",
    "special_symbols = r'([^<>a-z ])'#Matches special symbols such as © or ™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch tags, compatibility with lemmatise() \n",
    "def switchTag(tag):\n",
    "    if tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif (tag.startswith('J') or\n",
    "            tag.startswith('A')):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN \n",
    "\n",
    "#string_test='In gold, the open interest SURPRISINGLY ROSE BY A CONSIDERABLE 9126 CONTRACTS UP TO582,421 WITH THE GOOD SIZED RISE IN PRICE OF GOLD WITH YESTERDAY’S TRADING ($5.55). IN ANOTHER HUGE DEVELOPMENT, WE RECEIVED THE TOTAL NUMBER OF GOLD EFP’S ISSUED FOR WEDNESDAY AND IT TOTALED A HUMONGOUS SIZED 12,223 CONTRACTS OF WHICH FEBRUARY SAW 11,023 CONTRACTS ISSUED AND APRIL SAW THE ISSUANCE OF 1200 CONTRACTS.'\n",
    "#date_test  = '12/18/10 12/18/2020 12-18-10 12-18-2020 12/18/10 12/18/2020 12.18.10 12.18.2020 noise 12182010 december 18, 2010 janu 10th march 1st 3st january Dekjkj 10th  noise 10/20  noise noise 2020 10th january 2021'\n",
    "\n",
    "def clean_column(data, col_name):\n",
    "    for i, entry in zip(data[col_name].index, data[col_name]):    \n",
    "        #We first convert to lower case and replace punctuation with space such that dates can\n",
    "        #more easily be processed (eg. 10.12.2020 -> 10 12 2020 -> <NUMBER> <NUMBER> <NUMBER> instead of <NUMBER><NUMBER><DIGIT> or something)\n",
    "        cleaned = initial_cleaner.transform([entry])[0]\n",
    "        cleaned = general_cleaner.transform([cleaned])[0]\n",
    "        cleaned = re.sub(all_dates, '<date> ', cleaned)\n",
    "        cleaned = re.sub(special_symbols,'',cleaned)\n",
    "        cleaned = re.sub(multiple_chars, '', cleaned)\n",
    "        data.at[i, col_name] = cleaned\n",
    "\n",
    "def clean_data(data):\n",
    "    clean_column(data, \"content\")\n",
    "    clean_column(data, \"title\")\n",
    "    return data\n",
    "\n",
    "# Give n articles returns an array of n arrays. \n",
    "# Each containing vocalbulary for a paticular article\n",
    "def lemmatise_column(data, col_name):\n",
    "    retVal = []\n",
    "    for entry in data[col_name] : \n",
    "        buff = entry.replace('<date>','')\n",
    "        buff = buff.replace('<number>','')\n",
    "        buff = word_tokenize(buff)\n",
    "        buff = nltk.pos_tag(buff)\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmatised=[]\n",
    "        for index,word in enumerate(buff):\n",
    "            buff[index] = (word[0],switchTag(word[1]))\n",
    "            lemmatised.append(lemmatizer.lemmatize(buff[index][0],pos=buff[index][1]))\n",
    "        retVal.append(lemmatised)\n",
    "    return retVal\n",
    "\n",
    "def count_and_clean(data,col_name):\n",
    "    numbers=0\n",
    "    dates=0\n",
    "    urls=0\n",
    "    emails=0\n",
    "    phone_num=0\n",
    "    currencies=0\n",
    "    last_length = 0\n",
    "    tags = [\"<number>\",\"<number>\"]\n",
    "    for i, entry in zip(data[col_name].index, data[col_name]):\n",
    "        last_length = len(entry)\n",
    "        cleaned = entry.replace('<number>',\" \")\n",
    "        numbers = last_length-len(cleaned)\n",
    "        last_length = len(cleaned)\n",
    "\n",
    "        cleaned = entry.replace('<date>',\" \")\n",
    "        dates = last_length-len(cleaned)\n",
    "        last_length = len(cleaned)\n",
    "\n",
    "        cleaned = entry.replace('<url>',\" \")\n",
    "        urls = last_length-len(cleaned)\n",
    "        last_length = len(cleaned)\n",
    "\n",
    "        cleaned = entry.replace('<email>',\" \")\n",
    "        emails = last_length-len(cleaned)\n",
    "        last_length = len(cleaned)\n",
    "\n",
    "        cleaned = entry.replace('<number>',\" \")\n",
    "        numbers = last_length-len(cleaned)\n",
    "        last_length = len(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "wfs = WordFreqSet(\"content\", data)\n",
    "data = clean_data(data)\n",
    "# count dates and number \n",
    "df = pd.DataFrame(data, columns=['domain','type','url','content','title','authors', 'meta_keywords', 'meta_description', 'tags'])\n",
    "data = lemmatise_column(data, \"content\")\n",
    "wfs = WordFreqSet(\"content\", data)\n",
    "print(wfs)\n",
    "#df2 = df.assign(WordFreq = wfs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lematised_data = lemmatise(data, \"content\")\n",
    "\n",
    "#Freq Distribution for all articles\n",
    "dsitr = FreqDist(lemmatised_data)\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3\n",
    "\n",
    "We have observed that all fake news articles have been distributed by beforeitsnews.com.\n",
    "\n",
    "Besides that we have observed that no fake news articles contains meta_keywords or meta_description.\n",
    "\n",
    "We also observed that Conspiracy, hate and clickbait articles have very high author distribution. \n",
    "\n",
    "Hate articles tends to have longer sentences most likely due to rants.\n",
    "\n",
    "Reliable data seems to lower word length count per sentence and longer sentence length\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Task 4\n",
    "# Initialize Group SubString\n",
    "group_nr = 14\n",
    "group_substring_raw = \"ABCDEFGHIJKLMNOPRSTUVWZABCDEFGHIJKLMNOPRSTUVWZ\"[group_nr%23:group_nr%23+10]\n",
    "group_substring = \"\"\n",
    "\n",
    "for letter in np.sort(list(group_substring_raw)):\n",
    "    group_substring += letter\n",
    "    \n",
    "print(group_substring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Get main page and add subpage (according to group_substring) urls to list\n",
    "response = requests.get('https://en.wikinews.org/wiki/Category:Politics_and_conflicts')\n",
    "contents = response.text\n",
    "\n",
    "soup = BeautifulSoup(contents, 'html.parser')\n",
    "\n",
    "subpages = []\n",
    "for a in soup.find_all('a', href=True):\n",
    "    for letter in group_substring:\n",
    "        if \"conflicts&from=\"+letter in a[\"href\"]:\n",
    "            if not a[\"href\"] in subpages:\n",
    "                subpages.append(a[\"href\"])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of articles on THE FIRST PAGE ONLY, NEED TO FIX LATER\n",
    "Articles = []\n",
    "for url in subpages:\n",
    "    response = requests.get(url)\n",
    "    contents = response.text\n",
    "\n",
    "    soup = BeautifulSoup(contents, 'html.parser')\n",
    "\n",
    "    allGroups = soup.find_all(\"div\",attrs={\"class\":\"mw-category-group\"})\n",
    "    for n in allGroups:\n",
    "        if \"<h3>\"+url[-1]+\"</h3>\" in str(n) and \"<ul><li><a\" in str(n):\n",
    "            pages = n\n",
    "            break\n",
    "    ul = re.findall('\\/wiki.*(?=title)',str(pages))\n",
    "    for i in range(len(ul)):\n",
    "        ul[i] = \"https://en.wikinews.org\" + ul[i][:-2]\n",
    "\n",
    "    Articles.append(ul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GrabArticle(url):\n",
    "    response = requests.get(url)\n",
    "    contents = response.text\n",
    "\n",
    "    soup = BeautifulSoup(contents, 'html.parser')\n",
    "    \n",
    "    try: # Need to convert from \"MonthName Day, Year\" to \"Year-Month-Day\"\n",
    "        date = re.findall('[1-9]+.*[1-9]',str(soup.find(\"span\", attrs={\"id\":\"publishDate\"})))[0]\n",
    "    except:\n",
    "        try: \n",
    "            date = re.findall('[A-Z][a-z]+ [0-9]+, [0-9][0-9][0-9][0-9]',str(soup.find(\"div\", attrs={\"class\":\"mw-parser-output\"})))[0]\n",
    "        except:\n",
    "            date = \"NaN\"\n",
    "    try:\n",
    "        text = soup.get_text() #currently displays EVERYTHING on page, needs work\n",
    "    except:\n",
    "        text = \"NaN\"\n",
    "\n",
    "    \n",
    "    #srcs section needs work\n",
    "    srcs = []\n",
    "    try:\n",
    "        src = soup.find_all(\"span\",attrs={\"class\":\"sourceTemplate\"})\n",
    "        for n in src:\n",
    "            srcs.append(n)\n",
    "    except:\n",
    "        src = \"NaN\"\n",
    "\n",
    "    return date#,srcs,text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping article pages for data and adding to lists for dataframe creation\n",
    "urls = []\n",
    "dates = []\n",
    "for articles in Articles:\n",
    "    for url in articles:\n",
    "        urls.append(url)\n",
    "        dates.append(GrabArticle(url))\n",
    "        \n",
    "        \n",
    "Task4df = pd.DataFrame(data = {\"URL\" : urls, \"Date written\" : dates})"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
